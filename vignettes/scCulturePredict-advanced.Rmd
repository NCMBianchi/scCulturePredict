---
title: "Advanced Usage of scCulturePredict"
author: "NiccolÃ² Bianchi"
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc: true
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Advanced Usage of scCulturePredict}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>",
    fig.width = 8,
    fig.height = 6,
    warning = FALSE,
    message = FALSE
)
```

# Introduction

This vignette covers advanced usage of the `scCulturePredict` package, building on the basic concepts introduced in the introductory vignette. Here, we explore more sophisticated analyses, customization options, and performance optimizations.

```{r setup}
library(scCulturePredict)
library(Seurat)
library(dplyr)
```
library(ggplot2)
library(parallel)
library(doParallel)
library(foreach)
```

# Setup Example Data

We'll use the same example data as in the introduction vignette, but we'll process it differently:

```{r example_data}
# Get the path to example data
tenx_data_dir <- system.file("extdata", "example_data", package = "scCulturePredict")

# For demonstration purposes, create a mock Seurat object
counts <- read.csv(file.path(tenx_data_dir, "counts.csv"), row.names = 1)
metadata <- read.csv(file.path(tenx_data_dir, "metadata.csv"), row.names = 1)
seurat_object <- CreateSeuratObject(counts = as.matrix(counts), meta.data = metadata)
```

# Advanced scCulture() Usage

The `scCulture()` function provides several advanced options for power users who need more control over the analysis pipeline or want to optimize performance for large datasets.

## Progress Tracking and Parallel Processing

For large datasets, you can enable progress tracking and parallel processing to monitor and speed up the analysis:

```{r advanced_scumap, eval=FALSE}
# Advanced usage with all options
results <- scCulture(
    tenx_data_dir = tenx_data_dir,
    input_type = "10x",
    kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
    output_dir = "./advanced_results",
    mode = "build",
    experiment_id = "advanced_analysis",
    progress = TRUE, # Show detailed progress bar
    parallel = TRUE, # Enable parallel processing
    n_cores = 4, # Use 4 CPU cores
    perform_tsne = TRUE, # Include t-SNE analysis
    verbose = TRUE # Detailed progress messages
)

# The function returns comprehensive results
str(results, max.level = 2)
```

## Customizing Analysis Parameters

You can customize various aspects of the analysis by modifying the underlying functions:

```{r custom_parameters, eval=FALSE}
# Example: Custom analysis with modified parameters
results_custom <- scCulture(
    tenx_data_dir = tenx_data_dir,
    input_type = "10x",
    kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
    output_dir = "./custom_results",
    mode = "build",
    experiment_id = "custom_analysis",
    progress = TRUE,
    verbose = TRUE
)

# Access and modify the Seurat object
seurat_obj <- results_custom$seurat_object

# You can then run additional custom analyses on the results
# For example, additional clustering or custom visualizations
```

## Batch Processing Multiple Datasets

For processing multiple datasets, you can create a wrapper function:

```{r batch_processing, eval=FALSE}
# Function to process multiple datasets
process_multiple_datasets <- function(datasets, kegg_file, base_output_dir) {
    results_list <- list()

    for (i in seq_along(datasets)) {
        dataset_name <- names(datasets)[i]
        data_path <- datasets[[i]]

        cat(sprintf("Processing dataset %d/%d: %s\n", i, length(datasets), dataset_name))

        # Create unique output directory for each dataset
        output_dir <- file.path(base_output_dir, dataset_name)

        # Run scCulture analysis
        results_list[[dataset_name]] <- scCulture(
            tenx_data_dir = data_path,
            input_type = "10x",
            kegg_file = kegg_file,
            output_dir = output_dir,
            mode = "build",
            experiment_id = dataset_name,
            progress = FALSE, # Disable progress bar for batch processing
            parallel = TRUE,
            verbose = FALSE # Reduce verbosity for batch processing
        )
    }

    return(results_list)
}

# Example usage (not run)
# datasets <- list(
#   "experiment_1" = "/path/to/data1",
#   "experiment_2" = "/path/to/data2",
#   "experiment_3" = "/path/to/data3"
# )
#
# batch_results <- process_multiple_datasets(
#   datasets = datasets,
#   kegg_file = "pathway_file.keg",
#   base_output_dir = "./batch_results"
# )
```

# Advanced Preprocessing

## Batch Effect Correction

Batch effects can significantly impact single-cell analyses. Here, we demonstrate how to correct for batch effects:

```{r batch_correction, eval=FALSE}
# Identify batch variable in metadata
batch_var <- "batch"

# Run integration to correct for batch effects
seurat_list <- SplitObject(seurat_object, split.by = batch_var)

# Normalize and identify variable features for each dataset independently
seurat_list <- lapply(seurat_list, function(x) {
    x <- NormalizeData(x)
    x <- FindVariableFeatures(x, selection.method = "vst", nfeatures = 2000)
})

# Select integration features
features <- SelectIntegrationFeatures(object.list = seurat_list)

# Find integration anchors
anchors <- FindIntegrationAnchors(
    object.list = seurat_list,
    anchor.features = features
)

# Integrate data
seurat_integrated <- IntegrateData(anchorset = anchors)

# Switch to integrated assay for downstream analysis
DefaultAssay(seurat_integrated) <- "integrated"

# Standard preprocessing on integrated data
seurat_integrated <- ScaleData(seurat_integrated)
seurat_integrated <- RunPCA(seurat_integrated)
seurat_integrated <- RunUMAP(seurat_integrated, dims = 1:30)

# Visualize results with batch variable
p1 <- DimPlot(seurat_integrated, reduction = "umap", group.by = batch_var)
p2 <- DimPlot(seurat_integrated, reduction = "umap", group.by = "sample")
p1 + p2
```

## Custom QC and Filtering

More stringent QC can improve downstream analyses:

```{r custom_qc, eval=FALSE}
# Calculate QC metrics
seurat_object[["percent_mito"]] <- PercentageFeatureSet(seurat_object, pattern = "^MT-")
seurat_object[["percent_ribo"]] <- PercentageFeatureSet(seurat_object, pattern = "^RP[SL]")

# Visualize QC metrics
VlnPlot(seurat_object,
    features = c("nFeature_RNA", "nCount_RNA", "percent_mito", "percent_ribo"),
    ncol = 4, pt.size = 0.1
)

# Custom filtering
seurat_filtered <- subset(seurat_object,
    subset = nFeature_RNA > 500 &
        nFeature_RNA < 5000 &
        percent_mito < 15 &
        percent_ribo > 5
)
```

# Advanced KEGG Pathway Analysis

## Custom Pathway Definition

You can define custom pathways beyond standard KEGG pathways:

```{r custom_pathways, eval=FALSE}
# Define custom pathway gene sets
custom_pathways <- list(
    "Custom_Pathway_1" = c("ENSG00000001", "ENSG00000002", "ENSG00000003"),
    "Custom_Pathway_2" = c("ENSG00000004", "ENSG00000005", "ENSG00000006"),
    "Custom_Pathway_3" = c("ENSG00000007", "ENSG00000008", "ENSG00000009")
)

# Convert to format compatible with build_fingerprints
custom_pathways_df <- data.frame(
    pathway_id = rep(names(custom_pathways), sapply(custom_pathways, length)),
    gene_id = unlist(custom_pathways)
)

# Build custom pathway fingerprints
custom_fingerprints <- build_custom_fingerprints(
    seurat_object = seurat_object,
    pathway_gene_map = custom_pathways_df,
    verbose = TRUE
)
```

## Weighted Pathway Analysis

Incorporate gene importance weights in pathway analysis:

```{r weighted_pathway, eval=FALSE}
# Define gene weights (e.g., from differential expression analysis)
gene_weights <- data.frame(
    gene_id = rownames(seurat_object),
    weight = runif(nrow(seurat_object), 0, 1)
)

# Build weighted pathway fingerprints
weighted_fingerprints <- build_weighted_fingerprints(
    seurat_object = seurat_object,
    kegg_data = kegg_data,
    gene_weights = gene_weights,
    verbose = TRUE
)
```

# Performance Optimization

## Parallel Processing

For large datasets, parallel processing can significantly improve performance:

```{r parallel, eval=FALSE}
# Set up parallel backend
num_cores <- detectCores() - 1
registerDoParallel(cores = num_cores)

# Use parallel processing in pathway analysis
pathway_matrix <- build_fingerprints(
    seurat_object = seurat_object,
    kegg_data = kegg_data,
    use_parallel = TRUE,
    num_cores = num_cores,
    verbose = TRUE
)
```

## Memory Optimization

When working with large datasets, memory optimization is crucial:

```{r memory, eval=FALSE}
# Process data in chunks
chunk_size <- 1000 # cells per chunk
cell_chunks <- split(
    colnames(seurat_object),
    ceiling(seq_along(colnames(seurat_object)) / chunk_size)
)

# Process each chunk
results_list <- list()
for (i in seq_along(cell_chunks)) {
    if (verbose) message(sprintf("Processing chunk %d of %d", i, length(cell_chunks)))

    # Subset data
    chunk_obj <- subset(seurat_object, cells = cell_chunks[[i]])

    # Process chunk
    chunk_results <- process_chunk(chunk_obj)

    # Store results
    results_list[[i]] <- chunk_results

    # Explicit garbage collection
    gc()
}

# Combine results
combined_results <- combine_chunk_results(results_list)
```

# Advanced Prediction Models

## Ensemble Models

Combine multiple prediction methods for improved accuracy:

```{r ensemble, eval=FALSE}
# Create mock data for demonstration
set.seed(42)
pathway_matrix <- matrix(rnorm(30 * 10), nrow = 30, ncol = 10)
rownames(pathway_matrix) <- paste0("Pathway", 1:30)
colnames(pathway_matrix) <- colnames(seurat_object)

signature_matrix <- matrix(rnorm(10 * 3), nrow = 10, ncol = 3)
rownames(signature_matrix) <- paste0("Pathway", 1:10)
colnames(signature_matrix) <- c("A", "B", "C")

# Run multiple prediction methods
similarity_results <- predict_by_similarity(
    pathway_matrix = pathway_matrix,
    signature_matrix = signature_matrix
)

svm_results <- predict_by_svm(
    pathway_matrix = pathway_matrix,
    seurat_object = seurat_object
)

# Create an ensemble prediction
ensemble_predictions <- create_ensemble_predictions(
    seurat_object = seurat_object,
    similarity_results = similarity_results,
    svm_results = svm_results,
    weights = c(similarity = 0.4, svm = 0.6)
)
```

## Custom SVM Parameters

Fine-tune SVM parameters for optimal performance:

```{r svm_params, eval=FALSE}
# Create custom SVM prediction with optimized parameters
custom_svm_results <- predict_by_svm_custom(
    pathway_matrix = pathway_matrix,
    seurat_object = seurat_object,
    kernel = "radial",
    cost = 10,
    gamma = 0.1,
    probability = TRUE,
    cross_validation = 5
)
```

# Advanced Evaluation

## Cross-validation Evaluation

Perform k-fold cross-validation for robust evaluation:

```{r cross_val, eval=FALSE}
# Perform k-fold cross-validation
k <- 5
cv_results <- cross_validate_predictions(
    seurat_object = seurat_object,
    pathway_matrix = pathway_matrix,
    method = "svm",
    k_folds = k,
    stratify_by = "sample",
    verbose = TRUE
)

# Visualize cross-validation results
cv_plot <- plot_cross_validation_results(
    cv_results = cv_results,
    metric = "accuracy",
    title = "5-fold Cross-validation Results"
)
print(cv_plot)
```

## Advanced Metrics

Calculate additional performance metrics:

```{r adv_metrics, eval=FALSE}
# Calculate advanced metrics
advanced_metrics <- calculate_advanced_metrics(
    true_labels = seurat_object$sample,
    predicted_labels = svm_results$predictions,
    predicted_probs = svm_results$probabilities,
    metrics = c("auc", "precision", "recall", "f1", "kappa", "mcc")
)

# Create detailed metrics visualization
# The create_evaluation_metrics_plot function is now in inst/extras/alternative_implementations.R
# metrics_plot <- create_evaluation_metrics_plot(
#     evaluation_results = advanced_metrics,
#     plot_type = "metrics",
#     title = "Advanced Evaluation Metrics"
# )
# print(metrics_plot)
```

# Integration with Other Tools

## Integration with SingleR

Integrate with SingleR for reference-based cell type annotation:

```{r singler, eval=FALSE}
# Load required packages
library(SingleR)
library(celldex)

# Get reference dataset
ref <- celldex::HumanPrimaryCellAtlasData()

# Run SingleR
singler_results <- SingleR(
    test = GetAssayData(seurat_object, slot = "data"),
    ref = ref,
    labels = ref$label.main
)

# Add SingleR annotations to Seurat object
seurat_object$singler_labels <- singler_results$labels

# Compare SingleR annotations with predicted conditions
comparison_plot <- create_annotation_comparison_plot(
    seurat_object = seurat_object,
    singler_column = "singler_labels",
    prediction_column = "svm_prediction",
    title = "SingleR vs SVM Predictions"
)
print(comparison_plot)
```

## Integration with Trajectory Analysis

Integrate with trajectory analysis tools:

```{r trajectory, eval=FALSE}
# Load required packages
library(slingshot)
library(tradeSeq)

# Run trajectory analysis
seurat_object <- RunPCA(seurat_object)
seurat_object <- RunUMAP(seurat_object, dims = 1:30)

# Get UMAP embedding
umap_embedding <- Embeddings(seurat_object, "umap")

# Get cluster info
clusters <- seurat_object$seurat_clusters

# Run Slingshot
slingshot_results <- slingshot(
    data = umap_embedding,
    clusterLabels = clusters
)

# Visualize trajectory
plot_trajectory <- function(umap_embedding, slingshot_results, color_by) {
    plot_df <- data.frame(
        UMAP_1 = umap_embedding[, 1],
        UMAP_2 = umap_embedding[, 2],
        color = color_by
    )

    # Create plot
    p <- ggplot(plot_df, aes(x = UMAP_1, y = UMAP_2, color = color)) +
        geom_point(size = 1) +
        theme_minimal() +
        labs(title = "Trajectory Analysis")

    # Add trajectory lines
    for (i in 1:length(slingshot_results@curves)) {
        curve_points <- slingshot_results@curves[[i]]$s[slingshot_results@curves[[i]]$ord, ]
        curve_df <- data.frame(
            UMAP_1 = curve_points[, 1],
            UMAP_2 = curve_points[, 2]
        )
        p <- p + geom_path(
            data = curve_df, aes(x = UMAP_1, y = UMAP_2),
            color = "black", size = 1
        )
    }

    return(p)
}

# Create trajectory plot
traj_plot <- plot_trajectory(
    umap_embedding = umap_embedding,
    slingshot_results = slingshot_results,
    color_by = seurat_object$sample
)
print(traj_plot)
```

# Pipeline Customization

## Creating a Custom Analysis Pipeline

Build a customized analysis pipeline that combines multiple steps:

```{r custom_pipeline, eval=FALSE}
# Define custom pipeline function
custom_analysis_pipeline <- function(
    tenx_data_dir,
    experiment_id,
    kegg_file,
    output_dir = "results",
    use_parallel = FALSE,
    num_cores = 1,
    verbose = TRUE) {
    # 1. Load data
    if (verbose) message("1. Loading data...")
    seurat_object <- load_10x_data(
        tenx_data_dir = tenx_data_dir,
        experiment_id = experiment_id,
        verbose = verbose
    )

    # 2. Preprocess data
    if (verbose) message("2. Preprocessing data...")
    seurat_object <- preprocess_data(
        seurat_object = seurat_object,
        verbose = verbose
    )

    # 3. Reduce dimensions
    if (verbose) message("3. Reducing dimensions...")
    seurat_object <- reduce_dimensions(
        seurat_object = seurat_object,
        verbose = verbose
    )

    # 4. Perform pathway analysis
    if (verbose) message("4. Performing pathway analysis...")
    kegg_data <- parse_kegg_keg(
        kegg_file = kegg_file,
        verbose = verbose
    )

    pathway_matrix <- build_fingerprints(
        seurat_object = seurat_object,
        kegg_data = kegg_data,
        use_parallel = use_parallel,
        num_cores = num_cores,
        verbose = verbose
    )

    # 5. Create signature matrix
    if (verbose) message("5. Creating signature matrix...")
    signature_matrix <- create_signature_matrix(
        seurat_object = seurat_object,
        pathway_matrix = pathway_matrix,
        verbose = verbose
    )

    # 6. Make predictions
    if (verbose) message("6. Making predictions...")
    similarity_results <- predict_by_similarity(
        pathway_matrix = pathway_matrix,
        signature_matrix = signature_matrix,
        verbose = verbose
    )

    svm_results <- predict_by_svm(
        pathway_matrix = pathway_matrix,
        seurat_object = seurat_object,
        verbose = verbose
    )

    # 7. Evaluate predictions
    if (verbose) message("7. Evaluating predictions...")
    evaluation_results <- evaluate_predictions(seurat_object)

    # 8. Create visualizations
    if (verbose) message("8. Creating visualizations...")
    if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

    # Save plots using ggplot2::ggsave
    # Note: save_visualization_plots has been moved to inst/extras/alternative_implementations.R

    # Save evaluation plots if they exist
    evaluation_plots <- create_evaluation_plots(
        seurat_object = seurat_object,
        results_dir = output_dir
    )

    # Save individual plots
    for (i in seq_along(evaluation_plots)) {
        filename <- file.path(output_dir, paste0(experiment_name, "_plot_", i, ".png"))
        ggplot2::ggsave(filename, evaluation_plots[[i]], width = 10, height = 8, dpi = 300)
    }

    # 9. Return results
    return(list(
        seurat_object = seurat_object,
        pathway_matrix = pathway_matrix,
        signature_matrix = signature_matrix,
        similarity_results = similarity_results,
        svm_results = svm_results,
        evaluation_results = evaluation_results
    ))
}

# Example usage (not run)
# results <- custom_analysis_pipeline(
#   tenx_data_dir = tenx_data_dir,
#   experiment_id = "example",
#   kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
#   output_dir = "custom_results",
#   use_parallel = TRUE,
#   num_cores = 4,
#   verbose = TRUE
# )
```

# Advanced SingleCellExperiment Support

The `scCulturePredict` package provides comprehensive support for SingleCellExperiment (SCE) objects, enabling seamless integration with Bioconductor workflows.

## Working with SCE Objects

```{r advanced_sce, eval=FALSE}
library(SingleCellExperiment)

# Create or load an SCE object
sce <- SingleCellExperiment(
    assays = list(counts = matrix(rpois(10000, lambda = 5), nrow = 1000, ncol = 10)),
    colData = DataFrame(
        sample = rep(c("Control", "Treatment"), 5),
        batch = rep(c("Batch1", "Batch2"), each = 5)
    )
)

# Run analysis with SCE object directly
results_sce <- scCulture(
    sce_data_path = sce,  # Pass object directly
    input_type = "sce",
    kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
    output_dir = "./sce_results",
    mode = "build",
    experiment_id = "sce_advanced",
    verbose = TRUE
)

# Or load from RDS file
results_sce_file <- scCulture(
    sce_data_path = "path/to/sce_data.rds",
    input_type = "sce",
    kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
    output_dir = "./sce_file_results",
    mode = "build",
    experiment_id = "sce_from_file",
    verbose = TRUE
)
```

## Cross-Dataset Predictions

One of the most powerful features is the ability to train models on one data format and apply them to another:

```{r cross_dataset_advanced, eval=FALSE}
# Scenario 1: Train on 10X, predict on SCE
# This is useful when you have labeled 10X reference data
# and unlabeled SCE query data

# Step 1: Build model with 10X data
model_10x <- scCulture(
    tenx_data_dir = "reference_10x_data/",
    input_type = "10x",
    kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
    output_dir = "./model_10x",
    mode = "build",
    experiment_id = "10x_reference",
    verbose = TRUE
)

# Step 2: Apply to SCE data
predictions_sce <- scCulture(
    sce_data_path = sce,  # or path to RDS file
    input_type = "sce",
    output_dir = "./predictions_on_sce",
    mode = "predict",
    fingerprint_file = model_10x$fingerprint_file,
    experiment_id = "sce_query",
    verbose = TRUE
)

# Scenario 2: Train on SCE, predict on 10X
# Useful when your reference data is in SCE format

# Step 1: Build model with SCE data
model_sce <- scCulture(
    sce_data_path = "reference_sce.rds",
    input_type = "sce",
    kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
    output_dir = "./model_sce",
    mode = "build",
    experiment_id = "sce_reference",
    verbose = TRUE
)

# Step 2: Apply to 10X data
predictions_10x <- scCulture(
    tenx_data_dir = "query_10x_data/",
    input_type = "10x",
    output_dir = "./predictions_on_10x",
    mode = "predict",
    fingerprint_file = model_sce$fingerprint_file,
    experiment_id = "10x_query",
    verbose = TRUE
)
```

## Batch Processing with Mixed Data Types

Process multiple datasets of different formats in a single pipeline:

```{r batch_mixed, eval=FALSE}
# Define datasets with their types
datasets <- list(
    list(name = "dataset1", path = "data/10x_1/", type = "10x"),
    list(name = "dataset2", path = "data/sce_1.rds", type = "sce"),
    list(name = "dataset3", path = "data/10x_2/", type = "10x"),
    list(name = "dataset4", path = "data/sce_2.rds", type = "sce")
)

# Process all datasets
results_mixed <- list()

for (dataset in datasets) {
    if (dataset$type == "10x") {
        results_mixed[[dataset$name]] <- scCulture(
            tenx_data_dir = dataset$path,
            input_type = "10x",
            kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
            output_dir = paste0("./results_", dataset$name),
            mode = "build",
            experiment_id = dataset$name,
            verbose = TRUE
        )
    } else if (dataset$type == "sce") {
        results_mixed[[dataset$name]] <- scCulture(
            sce_data_path = dataset$path,
            input_type = "sce",
            kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
            output_dir = paste0("./results_", dataset$name),
            mode = "build",
            experiment_id = dataset$name,
            verbose = TRUE
        )
    }
}

# Compare results across datasets
accuracy_comparison <- sapply(results_mixed, function(x) {
    x$evaluation_results$overall_accuracy
})
print(accuracy_comparison)
```

## Advanced SCE Integration

Leverage SCE-specific features while using scCulturePredict:

```{r sce_integration, eval=FALSE}
# Use existing dimensionality reductions from SCE
library(scater)
library(scran)

# Prepare SCE with existing analyses
sce <- logNormCounts(sce)
sce <- runPCA(sce)
sce <- runUMAP(sce)

# The package will automatically detect and use these reductions
results_with_reductions <- scCulture(
    sce_data_path = sce,
    input_type = "sce",
    kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
    output_dir = "./sce_with_reductions",
    mode = "build",
    experiment_id = "sce_integrated",
    verbose = TRUE
)

# Access the results as Seurat object for further analysis
seurat_from_sce <- results_with_reductions$seurat_object

# Or convert back to SCE if needed for downstream Bioconductor workflows
# (requires additional conversion code)
```

# Conclusion

This vignette demonstrated advanced usage scenarios for the `scCulturePredict` package, including batch effect correction, custom pathway analysis, performance optimization, advanced prediction models, integration with other single-cell analysis tools, and comprehensive SingleCellExperiment support. These techniques can help you extract more insights from your single-cell data and customize the analysis to your specific research questions.

# Session Info

```{r session_info}
sessionInfo()
```
