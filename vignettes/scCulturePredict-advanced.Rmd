---
title: "Advanced Usage of scCulturePredict"
author: "NiccolÃ² Bianchi"
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc: true
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Advanced Usage of scCulturePredict}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6,
  warning = FALSE,
  message = FALSE
)
```

# Introduction

This vignette covers advanced usage of the `scCulturePredict` package, building on the basic concepts introduced in the introductory vignette. Here, we explore more sophisticated analyses, customization options, and performance optimizations.

```{r setup}
library(scCulturePredict)
library(Seurat)
library(dplyr)
```
library(ggplot2)
library(parallel)
library(doParallel)
library(foreach)
```

# Setup Example Data

We'll use the same example data as in the introduction vignette, but we'll process it differently:

```{r example_data}
# Get the path to example data
data_dir <- system.file("extdata", "example_data", package = "scCulturePredict")

# For demonstration purposes, create a mock Seurat object
counts <- read.csv(file.path(data_dir, "counts.csv"), row.names = 1)
metadata <- read.csv(file.path(data_dir, "metadata.csv"), row.names = 1)
seurat_object <- CreateSeuratObject(counts = as.matrix(counts), meta.data = metadata)
```

# Advanced scumap() Usage

The `scumap()` function provides several advanced options for power users who need more control over the analysis pipeline or want to optimize performance for large datasets.

## Progress Tracking and Parallel Processing

For large datasets, you can enable progress tracking and parallel processing to monitor and speed up the analysis:

```{r advanced_scumap, eval=FALSE}
# Advanced usage with all options
results <- scumap(
  data_dir = data_dir,
  kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
  output_dir = "./advanced_results",
  experiment_id = "advanced_analysis",
  progress = TRUE,           # Show detailed progress bar
  parallel = TRUE,           # Enable parallel processing
  n_cores = 4,              # Use 4 CPU cores
  perform_tsne = TRUE,       # Include t-SNE analysis
  use_shell_script = FALSE,  # Use R-only data loading
  verbose = TRUE             # Detailed progress messages
)

# The function returns comprehensive results
str(results, max.level = 2)
```

## Customizing Analysis Parameters

You can customize various aspects of the analysis by modifying the underlying functions:

```{r custom_parameters, eval=FALSE}
# Example: Custom analysis with modified parameters
results_custom <- scumap(
  data_dir = data_dir,
  kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
  output_dir = "./custom_results",
  experiment_id = "custom_analysis",
  progress = TRUE,
  verbose = TRUE
)

# Access and modify the Seurat object
seurat_obj <- results_custom$seurat_object

# You can then run additional custom analyses on the results
# For example, additional clustering or custom visualizations
```

## Batch Processing Multiple Datasets

For processing multiple datasets, you can create a wrapper function:

```{r batch_processing, eval=FALSE}
# Function to process multiple datasets
process_multiple_datasets <- function(datasets, kegg_file, base_output_dir) {
  results_list <- list()

  for (i in seq_along(datasets)) {
    dataset_name <- names(datasets)[i]
    data_path <- datasets[[i]]

    cat(sprintf("Processing dataset %d/%d: %s\n", i, length(datasets), dataset_name))

    # Create unique output directory for each dataset
    output_dir <- file.path(base_output_dir, dataset_name)

    # Run scumap analysis
    results_list[[dataset_name]] <- scumap(
      data_dir = data_path,
      kegg_file = kegg_file,
      output_dir = output_dir,
      experiment_id = dataset_name,
      progress = FALSE,  # Disable progress bar for batch processing
      parallel = TRUE,
      verbose = FALSE    # Reduce verbosity for batch processing
    )
  }

  return(results_list)
}

# Example usage (not run)
# datasets <- list(
#   "experiment_1" = "/path/to/data1",
#   "experiment_2" = "/path/to/data2",
#   "experiment_3" = "/path/to/data3"
# )
#
# batch_results <- process_multiple_datasets(
#   datasets = datasets,
#   kegg_file = "pathway_file.keg",
#   base_output_dir = "./batch_results"
# )
```

# Advanced Preprocessing

## Batch Effect Correction

Batch effects can significantly impact single-cell analyses. Here, we demonstrate how to correct for batch effects:

```{r batch_correction, eval=FALSE}
# Identify batch variable in metadata
batch_var <- "batch"

# Run integration to correct for batch effects
seurat_list <- SplitObject(seurat_object, split.by = batch_var)

# Normalize and identify variable features for each dataset independently
seurat_list <- lapply(seurat_list, function(x) {
  x <- NormalizeData(x)
  x <- FindVariableFeatures(x, selection.method = "vst", nfeatures = 2000)
})

# Select integration features
features <- SelectIntegrationFeatures(object.list = seurat_list)

# Find integration anchors
anchors <- FindIntegrationAnchors(object.list = seurat_list,
                                 anchor.features = features)

# Integrate data
seurat_integrated <- IntegrateData(anchorset = anchors)

# Switch to integrated assay for downstream analysis
DefaultAssay(seurat_integrated) <- "integrated"

# Standard preprocessing on integrated data
seurat_integrated <- ScaleData(seurat_integrated)
seurat_integrated <- RunPCA(seurat_integrated)
seurat_integrated <- RunUMAP(seurat_integrated, dims = 1:30)

# Visualize results with batch variable
p1 <- DimPlot(seurat_integrated, reduction = "umap", group.by = batch_var)
p2 <- DimPlot(seurat_integrated, reduction = "umap", group.by = "sample")
p1 + p2
```

## Custom QC and Filtering

More stringent QC can improve downstream analyses:

```{r custom_qc, eval=FALSE}
# Calculate QC metrics
seurat_object[["percent_mito"]] <- PercentageFeatureSet(seurat_object, pattern = "^MT-")
seurat_object[["percent_ribo"]] <- PercentageFeatureSet(seurat_object, pattern = "^RP[SL]")

# Visualize QC metrics
VlnPlot(seurat_object, features = c("nFeature_RNA", "nCount_RNA", "percent_mito", "percent_ribo"),
        ncol = 4, pt.size = 0.1)

# Custom filtering
seurat_filtered <- subset(seurat_object,
                          subset = nFeature_RNA > 500 &
                                  nFeature_RNA < 5000 &
                                  percent_mito < 15 &
                                  percent_ribo > 5)
```

# Advanced KEGG Pathway Analysis

## Custom Pathway Definition

You can define custom pathways beyond standard KEGG pathways:

```{r custom_pathways, eval=FALSE}
# Define custom pathway gene sets
custom_pathways <- list(
  "Custom_Pathway_1" = c("ENSG00000001", "ENSG00000002", "ENSG00000003"),
  "Custom_Pathway_2" = c("ENSG00000004", "ENSG00000005", "ENSG00000006"),
  "Custom_Pathway_3" = c("ENSG00000007", "ENSG00000008", "ENSG00000009")
)

# Convert to format compatible with build_fingerprints
custom_pathways_df <- data.frame(
  pathway_id = rep(names(custom_pathways), sapply(custom_pathways, length)),
  gene_id = unlist(custom_pathways)
)

# Build custom pathway fingerprints
custom_fingerprints <- build_custom_fingerprints(
  seurat_object = seurat_object,
  pathway_gene_map = custom_pathways_df,
  verbose = TRUE
)
```

## Weighted Pathway Analysis

Incorporate gene importance weights in pathway analysis:

```{r weighted_pathway, eval=FALSE}
# Define gene weights (e.g., from differential expression analysis)
gene_weights <- data.frame(
  gene_id = rownames(seurat_object),
  weight = runif(nrow(seurat_object), 0, 1)
)

# Build weighted pathway fingerprints
weighted_fingerprints <- build_weighted_fingerprints(
  seurat_object = seurat_object,
  kegg_data = kegg_data,
  gene_weights = gene_weights,
  verbose = TRUE
)
```

# Performance Optimization

## Parallel Processing

For large datasets, parallel processing can significantly improve performance:

```{r parallel, eval=FALSE}
# Set up parallel backend
num_cores <- detectCores() - 1
registerDoParallel(cores = num_cores)

# Use parallel processing in pathway analysis
pathway_matrix <- build_fingerprints(
  seurat_object = seurat_object,
  kegg_data = kegg_data,
  use_parallel = TRUE,
  num_cores = num_cores,
  verbose = TRUE
)
```

## Memory Optimization

When working with large datasets, memory optimization is crucial:

```{r memory, eval=FALSE}
# Process data in chunks
chunk_size <- 1000  # cells per chunk
cell_chunks <- split(colnames(seurat_object),
                     ceiling(seq_along(colnames(seurat_object)) / chunk_size))

# Process each chunk
results_list <- list()
for (i in seq_along(cell_chunks)) {
  if (verbose) message(sprintf("Processing chunk %d of %d", i, length(cell_chunks)))

  # Subset data
  chunk_obj <- subset(seurat_object, cells = cell_chunks[[i]])

  # Process chunk
  chunk_results <- process_chunk(chunk_obj)

  # Store results
  results_list[[i]] <- chunk_results

  # Explicit garbage collection
  gc()
}

# Combine results
combined_results <- combine_chunk_results(results_list)
```

# Advanced Prediction Models

## Ensemble Models

Combine multiple prediction methods for improved accuracy:

```{r ensemble, eval=FALSE}
# Create mock data for demonstration
set.seed(42)
pathway_matrix <- matrix(rnorm(30 * 10), nrow = 30, ncol = 10)
rownames(pathway_matrix) <- paste0("Pathway", 1:30)
colnames(pathway_matrix) <- colnames(seurat_object)

signature_matrix <- matrix(rnorm(10 * 3), nrow = 10, ncol = 3)
rownames(signature_matrix) <- paste0("Pathway", 1:10)
colnames(signature_matrix) <- c("A", "B", "C")

# Run multiple prediction methods
similarity_results <- predict_by_similarity(
  pathway_matrix = pathway_matrix,
  signature_matrix = signature_matrix
)

svm_results <- predict_by_svm(
  pathway_matrix = pathway_matrix,
  seurat_object = seurat_object
)

# Create an ensemble prediction
ensemble_predictions <- create_ensemble_predictions(
  seurat_object = seurat_object,
  similarity_results = similarity_results,
  svm_results = svm_results,
  weights = c(similarity = 0.4, svm = 0.6)
)
```

## Custom SVM Parameters

Fine-tune SVM parameters for optimal performance:

```{r svm_params, eval=FALSE}
# Create custom SVM prediction with optimized parameters
custom_svm_results <- predict_by_svm_custom(
  pathway_matrix = pathway_matrix,
  seurat_object = seurat_object,
  kernel = "radial",
  cost = 10,
  gamma = 0.1,
  probability = TRUE,
  cross_validation = 5
)
```

# Advanced Evaluation

## Cross-validation Evaluation

Perform k-fold cross-validation for robust evaluation:

```{r cross_val, eval=FALSE}
# Perform k-fold cross-validation
k <- 5
cv_results <- cross_validate_predictions(
  seurat_object = seurat_object,
  pathway_matrix = pathway_matrix,
  method = "svm",
  k_folds = k,
  stratify_by = "sample",
  verbose = TRUE
)

# Visualize cross-validation results
cv_plot <- plot_cross_validation_results(
  cv_results = cv_results,
  metric = "accuracy",
  title = "5-fold Cross-validation Results"
)
print(cv_plot)
```

## Advanced Metrics

Calculate additional performance metrics:

```{r adv_metrics, eval=FALSE}
# Calculate advanced metrics
advanced_metrics <- calculate_advanced_metrics(
  true_labels = seurat_object$sample,
  predicted_labels = svm_results$predictions,
  predicted_probs = svm_results$probabilities,
  metrics = c("auc", "precision", "recall", "f1", "kappa", "mcc")
)

# Create detailed metrics visualization
metrics_plot <- create_evaluation_metrics_plot(
  evaluation_results = advanced_metrics,
  plot_type = "metrics",
  title = "Advanced Evaluation Metrics"
)
print(metrics_plot)
```

# Integration with Other Tools

## Integration with SingleR

Integrate with SingleR for reference-based cell type annotation:

```{r singler, eval=FALSE}
# Load required packages
library(SingleR)
library(celldex)

# Get reference dataset
ref <- celldex::HumanPrimaryCellAtlasData()

# Run SingleR
singler_results <- SingleR(
  test = GetAssayData(seurat_object, slot = "data"),
  ref = ref,
  labels = ref$label.main
)

# Add SingleR annotations to Seurat object
seurat_object$singler_labels <- singler_results$labels

# Compare SingleR annotations with predicted conditions
comparison_plot <- create_annotation_comparison_plot(
  seurat_object = seurat_object,
  singler_column = "singler_labels",
  prediction_column = "svm_prediction",
  title = "SingleR vs SVM Predictions"
)
print(comparison_plot)
```

## Integration with Trajectory Analysis

Integrate with trajectory analysis tools:

```{r trajectory, eval=FALSE}
# Load required packages
library(slingshot)
library(tradeSeq)

# Run trajectory analysis
seurat_object <- RunPCA(seurat_object)
seurat_object <- RunUMAP(seurat_object, dims = 1:30)

# Get UMAP embedding
umap_embedding <- Embeddings(seurat_object, "umap")

# Get cluster info
clusters <- seurat_object$seurat_clusters

# Run Slingshot
slingshot_results <- slingshot(
  data = umap_embedding,
  clusterLabels = clusters
)

# Visualize trajectory
plot_trajectory <- function(umap_embedding, slingshot_results, color_by) {
  plot_df <- data.frame(
    UMAP_1 = umap_embedding[,1],
    UMAP_2 = umap_embedding[,2],
    color = color_by
  )

  # Create plot
  p <- ggplot(plot_df, aes(x = UMAP_1, y = UMAP_2, color = color)) +
    geom_point(size = 1) +
    theme_minimal() +
    labs(title = "Trajectory Analysis")

  # Add trajectory lines
  for (i in 1:length(slingshot_results@curves)) {
    curve_points <- slingshot_results@curves[[i]]$s[slingshot_results@curves[[i]]$ord, ]
    curve_df <- data.frame(
      UMAP_1 = curve_points[,1],
      UMAP_2 = curve_points[,2]
    )
    p <- p + geom_path(data = curve_df, aes(x = UMAP_1, y = UMAP_2),
                      color = "black", size = 1)
  }

  return(p)
}

# Create trajectory plot
traj_plot <- plot_trajectory(
  umap_embedding = umap_embedding,
  slingshot_results = slingshot_results,
  color_by = seurat_object$sample
)
print(traj_plot)
```

# Pipeline Customization

## Creating a Custom Analysis Pipeline

Build a customized analysis pipeline that combines multiple steps:

```{r custom_pipeline, eval=FALSE}
# Define custom pipeline function
custom_analysis_pipeline <- function(
  data_dir,
  experiment_id,
  kegg_file,
  output_dir = "results",
  use_parallel = FALSE,
  num_cores = 1,
  verbose = TRUE
) {
  # 1. Load data
  if (verbose) message("1. Loading data...")
  seurat_object <- load_data(
    data_dir = data_dir,
    experiment_id = experiment_id,
    verbose = verbose
  )

  # 2. Preprocess data
  if (verbose) message("2. Preprocessing data...")
  seurat_object <- preprocess_data(
    seurat_object = seurat_object,
    verbose = verbose
  )

  # 3. Reduce dimensions
  if (verbose) message("3. Reducing dimensions...")
  seurat_object <- reduce_dimensions(
    seurat_object = seurat_object,
    verbose = verbose
  )

  # 4. Perform pathway analysis
  if (verbose) message("4. Performing pathway analysis...")
  kegg_data <- parse_kegg_keg(
    kegg_file = kegg_file,
    verbose = verbose
  )

  pathway_matrix <- build_fingerprints(
    seurat_object = seurat_object,
    kegg_data = kegg_data,
    use_parallel = use_parallel,
    num_cores = num_cores,
    verbose = verbose
  )

  # 5. Create signature matrix
  if (verbose) message("5. Creating signature matrix...")
  signature_matrix <- create_signature_matrix(
    seurat_object = seurat_object,
    pathway_matrix = pathway_matrix,
    verbose = verbose
  )

  # 6. Make predictions
  if (verbose) message("6. Making predictions...")
  similarity_results <- predict_by_similarity(
    pathway_matrix = pathway_matrix,
    signature_matrix = signature_matrix,
    verbose = verbose
  )

  svm_results <- predict_by_svm(
    pathway_matrix = pathway_matrix,
    seurat_object = seurat_object,
    verbose = verbose
  )

  # 7. Evaluate predictions
  if (verbose) message("7. Evaluating predictions...")
  evaluation_results <- evaluate_predictions(seurat_object)

  # 8. Create visualizations
  if (verbose) message("8. Creating visualizations...")
  if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

  save_visualization_plots(
    seurat_object = seurat_object,
    evaluation_results = evaluation_results,
    output_dir = output_dir,
    prefix = experiment_id,
    verbose = verbose
  )

  # 9. Return results
  return(list(
    seurat_object = seurat_object,
    pathway_matrix = pathway_matrix,
    signature_matrix = signature_matrix,
    similarity_results = similarity_results,
    svm_results = svm_results,
    evaluation_results = evaluation_results
  ))
}

# Example usage (not run)
# results <- custom_analysis_pipeline(
#   data_dir = data_dir,
#   experiment_id = "example",
#   kegg_file = system.file("extdata", "kegg", "example_pathway.keg", package = "scCulturePredict"),
#   output_dir = "custom_results",
#   use_parallel = TRUE,
#   num_cores = 4,
#   verbose = TRUE
# )
```

# Conclusion

This vignette demonstrated advanced usage scenarios for the `scCulturePredict` package, including batch effect correction, custom pathway analysis, performance optimization, advanced prediction models, and integration with other single-cell analysis tools. These techniques can help you extract more insights from your single-cell data and customize the analysis to your specific research questions.

# Session Info

```{r session_info}
sessionInfo()
```
